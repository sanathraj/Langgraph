{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1484fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted? False\n",
      "Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 1) Define the shape of our state\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "# 2) Node: ask for name\n",
    "def ask_name(state: State):\n",
    "    return {\"messages\": [AIMessage(content=\"What’s your name?\")]}\n",
    "\n",
    "# 3) Node: pass-through human node\n",
    "def human_pass(state: State):\n",
    "    # after resume, state[\"messages\"] already contains your HumanMessage\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# 4) Node: echo greeting\n",
    "def greet(state: State):\n",
    "    name_msg = state[\"messages\"][-1]   # this is the HumanMessage\n",
    "    return {\"messages\": [AIMessage(content=f\"Hello, {name_msg.content}!\")]}\n",
    "\n",
    "# 5) Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask\",   ask_name)\n",
    "builder.add_node(\"human\", human_pass)\n",
    "builder.add_node(\"greet\", greet)\n",
    "builder.add_edge(\"ask\",   \"human\")\n",
    "builder.add_edge(\"human\", \"greet\")\n",
    "builder.set_entry_point(\"ask\")\n",
    "\n",
    "# 6) Compile with interrupt on the human node\n",
    "graph = builder.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    interrupt_before=[\"human\"],\n",
    ")\n",
    "\n",
    "# 7) Prepare a config with a thread_id so MemorySaver will work\n",
    "config = {\"thread_id\": \"human_loop_example\"}\n",
    "\n",
    "# 8) First invocation — runs ask_name, then stops at \"human\"\n",
    "result = graph.invoke({}, config=config)\n",
    "print(\"Interrupted?\", \"__interrupt__\" in result)   # → True\n",
    "\n",
    "# 9) Resume by injecting your HumanMessage via Command\n",
    "resume = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}    # your name here\n",
    "})\n",
    "final = graph.invoke(resume, config=config)\n",
    "\n",
    "# 10) Inspect the bot’s reply\n",
    "print(final[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e96c408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted? False\n"
     ]
    }
   ],
   "source": [
    "# … everything else is exactly as before …\n",
    "\n",
    "# 7) First invocation — tell it to stream updates so it stops at your human node\n",
    "result = graph.invoke(\n",
    "    {}, \n",
    "    config=config, \n",
    "    stream_mode=\"updates\"       # ← this line makes it yield an interrupt\n",
    ")\n",
    "print(\"Interrupted?\", \"__interrupt__\" in result)   # → True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6529d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "# 8) Now it’s paused at \"human\". Resume as before:\n",
    "resume = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}\n",
    "})\n",
    "final = graph.invoke(resume, config=config)\n",
    "print(final[\"messages\"][-1].content)  # → \"Hello, Alice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80adc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted? False\n",
      "Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "# … after your compile() and config setup …\n",
    "\n",
    "# 1) Invoke with an explicit interrupt_before\n",
    "result = graph.invoke(\n",
    "    {}, \n",
    "    config=config, \n",
    "    interrupt_before=[\"human\"]\n",
    ")\n",
    "\n",
    "print(\"Interrupted?\", \"__interrupt__\" in result)   # → True\n",
    "# result[\"__interrupt__\"] will tell you it’s waiting at \"human\"\n",
    "\n",
    "# 2) Now resume as before:\n",
    "resume = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}    # your human input\n",
    "})\n",
    "final = graph.invoke(resume, config=config)\n",
    "\n",
    "print(final[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6288b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted? False\n",
      "Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 1) Define your state\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "# 2) Asks for the user’s name\n",
    "def ask_name(state: State):\n",
    "    return {\"messages\": [AIMessage(content=\"What’s your name?\")]}\n",
    "\n",
    "# 3) A pass‑through “human” node\n",
    "def human_pass(state: State):\n",
    "    # after resume, state[\"messages\"] includes the HumanMessage you injected\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# 4) Greets based on the human input\n",
    "def greet(state: State):\n",
    "    last: HumanMessage = state[\"messages\"][-1]\n",
    "    return {\"messages\": [AIMessage(content=f\"Hello, {last.content}!\")]}\n",
    "\n",
    "# 5) Build your graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask\",   ask_name)\n",
    "builder.add_node(\"human\", human_pass)\n",
    "builder.add_node(\"greet\", greet)\n",
    "builder.add_edge(\"ask\",   \"human\")\n",
    "builder.add_edge(\"human\", \"greet\")\n",
    "builder.set_entry_point(\"ask\")\n",
    "\n",
    "# 6) Compile — **no** interrupt configured here\n",
    "graph = builder.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    ")\n",
    "\n",
    "# 7) Provide a thread_id so the MemorySaver can checkpoint\n",
    "config = {\"thread_id\": \"human_loop_example\"}\n",
    "\n",
    "# 8) **Invoke** with the interrupt directive **at** invoke-time\n",
    "result = graph.invoke(\n",
    "    {}, \n",
    "    config=config, \n",
    "    interrupt_before=[\"human\"]\n",
    ")\n",
    "print(\"Interrupted?\", \"__interrupt__\" in result)   # → True\n",
    "\n",
    "# 9) Now resume by injecting your human reply\n",
    "resume = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}    # ← swap in any name you like\n",
    "})\n",
    "final = graph.invoke(resume, config=config)\n",
    "\n",
    "# 10) Check the final greeting\n",
    "print(final[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fa46f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interrupt_chunk:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo interrupt found—did the graph skip the human node?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏸ Paused at node:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43minterrupt_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__interrupt__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 9) Resume with a name\u001b[39;00m\n\u001b[0;32m     63\u001b[0m resume \u001b[38;5;241m=\u001b[39m Command(resume\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     66\u001b[0m })\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 1) Define your state\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "# 2) Node: ask for name\n",
    "def ask_name(state: State):\n",
    "    return {\"messages\": [AIMessage(content=\"What’s your name?\")]}\n",
    "\n",
    "# 3) Node: pass‑through human\n",
    "def human_pass(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# 4) Node: greeting\n",
    "def greet(state: State):\n",
    "    last: HumanMessage = state[\"messages\"][-1]\n",
    "    return {\"messages\": [AIMessage(content=f\"Hello, {last.content}!\")]}\n",
    "\n",
    "# 5) Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask\",   ask_name)\n",
    "builder.add_node(\"human\", human_pass)\n",
    "builder.add_node(\"greet\", greet)\n",
    "builder.add_edge(\"ask\",   \"human\")\n",
    "builder.add_edge(\"human\", \"greet\")\n",
    "builder.set_entry_point(\"ask\")\n",
    "\n",
    "# 6) Compile (no interrupt flags here)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 7) Config for MemorySaver\n",
    "config = {\"thread_id\": \"human_loop_example\"}\n",
    "\n",
    "# 8) Invoke with streaming + interrupt_before\n",
    "stream_result = graph.invoke(\n",
    "    {},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\",\n",
    "    interrupt_before=[\"human\"]\n",
    ")\n",
    "\n",
    "# If it came back as a tuple, unpack the chunks list:\n",
    "if isinstance(stream_result, tuple) and len(stream_result) >= 1:\n",
    "    chunks = stream_result[0]\n",
    "else:\n",
    "    chunks = stream_result  # might already be a list\n",
    "\n",
    "# Now find the interrupt chunk\n",
    "interrupt_chunk = next(\n",
    "    (c for c in chunks if isinstance(c, dict) and \"__interrupt__\" in c),\n",
    "    None\n",
    ")\n",
    "if not interrupt_chunk:\n",
    "    raise RuntimeError(\"No interrupt found—did the graph skip the human node?\")\n",
    "print(\"⏸ Paused at node:\", interrupt_chunk[\"__interrupt__\"][\"node\"])\n",
    "\n",
    "# 9) Resume with a name\n",
    "resume = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}\n",
    "})\n",
    "resume_result = graph.invoke(resume, config=config, stream_mode=\"updates\")\n",
    "\n",
    "# Again unpack if needed\n",
    "if isinstance(resume_result, tuple) and len(resume_result) >= 1:\n",
    "    continue_chunks = resume_result[0]\n",
    "else:\n",
    "    continue_chunks = resume_result\n",
    "\n",
    "# Pull out the final greeting\n",
    "greet_chunk = next(\n",
    "    (c for c in continue_chunks if isinstance(c, dict) and \"messages\" in c),\n",
    "    None\n",
    ")\n",
    "print(\"🤖\", greet_chunk[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ee78ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "interrupt() got an unexpected keyword argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m graph \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 1) Run once—you’ll get back a dict with \"__interrupt__\" in it\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaused:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 2) Now resume by sending your input\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2844\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[0;32m   2841\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2842\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2844\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2851\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2854\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2857\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2534\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2532\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2533\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2534\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmpty\u001b[49m\n\u001b[0;32m   2543\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2544\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\LangGraph-Udemy-Course\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mhuman_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhuman_node\u001b[39m(state):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# this will immediately return an interrupt at runtime\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(waiting for human input)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: interrupt() got an unexpected keyword argument 'prompt'",
      "\u001b[0mDuring task with name 'human' and id '983f2d61-ad05-9bd6-1166-6e78fd5b871f'"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "def ask_name(state):\n",
    "    return {\"messages\": [AIMessage(content=\"What’s your name?\")]}\n",
    "\n",
    "def human_node(state):\n",
    "    # this will immediately return an interrupt at runtime\n",
    "    return interrupt(\n",
    "        state,\n",
    "        prompt={\"role\":\"assistant\",\"content\":\"(waiting for human input)\"}\n",
    "    )\n",
    "\n",
    "def greet(state):\n",
    "    last: HumanMessage = state[\"messages\"][-1]\n",
    "    return {\"messages\": [AIMessage(content=f\"Hello, {last.content}!\")]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask\",   ask_name)\n",
    "builder.add_node(\"human\", human_node)\n",
    "builder.add_node(\"greet\", greet)\n",
    "builder.add_edge(\"ask\",   \"human\")\n",
    "builder.add_edge(\"human\", \"greet\")\n",
    "builder.set_entry_point(\"ask\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 1) Run once—you’ll get back a dict with \"__interrupt__\" in it\n",
    "result = graph.invoke({}, interrupt_before=[])\n",
    "print(\"Paused:\", \"__interrupt__\" in result)\n",
    "\n",
    "# 2) Now resume by sending your input\n",
    "resume = Command(resume={\"action\":\"update\",\"data\":{\"content\":\"Alice\"}})\n",
    "final = graph.invoke(resume)\n",
    "print(final[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1626fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted? True\n",
      "What’s your name?\n",
      "🤖 Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "# 1. Define state schema\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "\n",
    "# 2. Ask the user\n",
    "def ask_name(state: State):\n",
    "    return {\"messages\": [AIMessage(content=\"What’s your name?\")]}\n",
    "\n",
    "\n",
    "# 3. Pause for human input using `interrupt`\n",
    "def wait_for_human(state: State):\n",
    "    return interrupt(state)  # triggers pause\n",
    "\n",
    "\n",
    "# 4. Respond with greeting\n",
    "def greet(state: State):\n",
    "    human_msg = state[\"messages\"][-1]  # last message is the human's response\n",
    "    return {\"messages\": [AIMessage(content=f\"Hello, {human_msg.content}!\")]}\n",
    "\n",
    "\n",
    "# 5. Build LangGraph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask\", ask_name)\n",
    "builder.add_node(\"wait\", wait_for_human)\n",
    "builder.add_node(\"greet\", greet)\n",
    "\n",
    "builder.add_edge(\"ask\", \"wait\")\n",
    "builder.add_edge(\"wait\", \"greet\")\n",
    "builder.set_entry_point(\"ask\")\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 6. Initial run (pauses at 'wait' via `interrupt`)\n",
    "config = {\"thread_id\": \"human_demo_123\"}\n",
    "result = graph.invoke({}, config=config)\n",
    "\n",
    "# Should contain \"__interrupt__\"\n",
    "print(\"Interrupted?\", \"__interrupt__\" in result)\n",
    "print(result[\"messages\"][-1].content)  # Should be \"What’s your name?\"\n",
    "\n",
    "# 7. Resume manually by injecting human response\n",
    "resume_cmd = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": {\"content\": \"Alice\"}  # Replace with any name\n",
    "})\n",
    "final_result = graph.invoke(resume_cmd, config=config)\n",
    "\n",
    "# Final message\n",
    "print(\"🤖\", final_result[\"messages\"][-1].content)  # → \"Hello, Alice!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e9594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "resume_cmd = Command(resume={\n",
    "    \"action\": \"update\",\n",
    "    \"data\": HumanMessage(content=\"Alice\")   # ✅ wrap properly\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea52a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Hello, What’s your name?!\n"
     ]
    }
   ],
   "source": [
    "final_result = graph.invoke(resume_cmd, config=config)\n",
    "print(\"🤖\", final_result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19719718",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToolNode' from 'langgraph.prebuilt' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, START\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_messages\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolNode, tools_condition\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Command, interrupt\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemorySaver\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ToolNode' from 'langgraph.prebuilt' (unknown location)"
     ]
    }
   ],
   "source": [
    "# pip install langgraph langchain langchain-core langchain-tavily typing_extensions\n",
    "\n",
    "import uuid\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# 1) State with message history\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 2) LLM + tool that pauses for human input\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance/approval from a human.\"\"\"\n",
    "    resp = interrupt({\"query\": query})         # pause here\n",
    "    return resp[\"data\"]                         # resume fills this\n",
    "\n",
    "tools = [human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    msg = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert len(msg.tool_calls) <= 1             # avoid dup calls on resume\n",
    "    return {\"messages\": [msg]}\n",
    "\n",
    "# 3) Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chatbot\")\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph = builder.compile(checkpointer=InMemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 4) Run – it will pause when tool is invoked\n",
    "events = graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"Please ask an expert for tips.\"}]},\n",
    "                      config, stream_mode=\"values\")\n",
    "for _ in events:\n",
    "    pass  # you'll receive an interrupt with {\"query\": ...}\n",
    "\n",
    "# 5) Resume with human input\n",
    "human_text = \"Expert says: prefer LangGraph for reliable agents.\"\n",
    "graph.stream(Command(resume={\"data\": human_text}), config, stream_mode=\"values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2deb08d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpong:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m tools \u001b[38;5;241m=\u001b[39m [ping]\n\u001b[1;32m---> 18\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\u001b[38;5;241m.\u001b[39mbind_tools(tools)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent\u001b[39m(state: State):\n\u001b[0;32m     21\u001b[0m     ai \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "@tool\n",
    "def ping(x: int) -> str:\n",
    "    \"\"\"Return pong with x.\"\"\"\n",
    "    return f\"pong:{x}\"\n",
    "\n",
    "tools = [ping]\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7).bind_tools(tools)\n",
    "\n",
    "def agent(state: State):\n",
    "    ai = llm.invoke(state[\"messages\"])\n",
    "    out = [ai]\n",
    "    # Manually execute any tool calls\n",
    "    for tc in ai.tool_calls or []:\n",
    "        func = {t.name: t for t in tools}[tc[\"name\"]]\n",
    "        result = func.invoke(tc[\"args\"])\n",
    "        out.append(ToolMessage(tool_call_id=tc[\"id\"], content=str(result)))\n",
    "    return {\"messages\": out}\n",
    "\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"agent\", agent)\n",
    "g.add_edge(START, \"agent\")\n",
    "g.add_edge(\"agent\", END)\n",
    "graph = g.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7339847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting run (expect an interrupt asking for approval) ---\n",
      "\n",
      "--- Resuming with human approval: yes ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CompiledStateGraph' object has no attribute 'resume'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Resuming with human approval:\u001b[39m\u001b[38;5;124m\"\u001b[39m, human_reply, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# .resume sends the value back into the most recent interrupt for this thread\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m resumed_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume\u001b[49m(human_reply, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 3) Final state / messages\u001b[39;00m\n\u001b[0;32m     81\u001b[0m final_state \u001b[38;5;241m=\u001b[39m resumed_events[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m resumed_events \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CompiledStateGraph' object has no attribute 'resume'"
     ]
    }
   ],
   "source": [
    "# pip install -U langgraph langchain-core\n",
    "\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "# ---------- State definition ----------\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # conversation transcript\n",
    "    approved: Literal[\"yes\", \"no\", \"unknown\"]  # human approval flag\n",
    "\n",
    "# ---------- Nodes ----------\n",
    "def plan(state: GraphState) -> GraphState:\n",
    "    \"\"\"Draft a plan based on the last user message.\"\"\"\n",
    "    last_user = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
    "    proposal = f\"Plan: I will carry out the request -> {last_user}. Approve? (yes/no)\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=proposal)]\n",
    "    }\n",
    "\n",
    "def need_approval(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Pause the graph and ask for human approval.\n",
    "    The first time this runs, it interrupts. When resumed, it receives the human's answer.\n",
    "    \"\"\"\n",
    "    # Ask the human. The value you pass to `resume(...)` later will be returned here.\n",
    "    answer = interrupt({\"question\": \"Approve the plan? Reply 'yes' or 'no'.\"})\n",
    "    # normalize\n",
    "    ans = str(answer).strip().lower()\n",
    "    if ans not in (\"yes\", \"no\"):\n",
    "        ans = \"no\"\n",
    "    return {\"approved\": ans}\n",
    "\n",
    "def execute(state: GraphState) -> GraphState:\n",
    "    \"\"\"Do something based on approval.\"\"\"\n",
    "    if state.get(\"approved\", \"unknown\") != \"yes\":\n",
    "        return {\"messages\": [AIMessage(\"Execution cancelled based on your decision.\")]}\n",
    "    return {\"messages\": [AIMessage(\"Executing the approved plan now ✅\")]}\n",
    "\n",
    "# ---------- Graph wiring ----------\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"plan\", plan)\n",
    "builder.add_node(\"need_approval\", need_approval)\n",
    "builder.add_node(\"execute\", execute)\n",
    "\n",
    "builder.add_edge(START, \"plan\")\n",
    "builder.add_edge(\"plan\", \"need_approval\")\n",
    "builder.add_edge(\"need_approval\", \"execute\")\n",
    "builder.add_edge(\"execute\", END)\n",
    "\n",
    "# Checkpointer is required to resume the same thread\n",
    "memory = MemorySaver()\n",
    "app = builder.compile(checkpointer=memory)\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "# Give every conversation a stable thread_id so you can resume it after an interrupt.\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "# 1) Kick off the run. Use .stream so we can detect the interrupt event.\n",
    "print(\"\\n--- Starting run (expect an interrupt asking for approval) ---\")\n",
    "events = list(app.stream(\n",
    "    {\"messages\": [HumanMessage(\"Book a table for two at 7pm and send me a confirmation.\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",   # yields state snapshots as it advances\n",
    "))\n",
    "\n",
    "# At this point, the graph has paused at `need_approval`.\n",
    "# You can inspect `events` if you want, but the important step is to RESUME with human input.\n",
    "\n",
    "# 2) Human responds out-of-band (e.g., UI form, CLI input). We'll hardcode \"yes\" here:\n",
    "human_reply = \"yes\"\n",
    "\n",
    "print(\"\\n--- Resuming with human approval:\", human_reply, \"---\")\n",
    "# .resume sends the value back into the most recent interrupt for this thread\n",
    "resumed_events = list(app.resume(human_reply, config, stream_mode=\"values\"))\n",
    "\n",
    "# 3) Final state / messages\n",
    "final_state = resumed_events[-1] if resumed_events else {}\n",
    "print(\"\\n--- Final assistant messages ---\")\n",
    "for msg in final_state.get(\"messages\", []):\n",
    "    role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "    print(f\"{role}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e44121ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting run (expect an interrupt asking for approval) ---\n",
      "\n",
      "--- Resuming with human approval: yes ---\n",
      "\n",
      "--- Final assistant messages ---\n",
      "User: Book a table for two at 7pm and send me a confirmation.\n",
      "Assistant: Plan: I will carry out the request -> Book a table for two at 7pm and send me a confirmation.. Approve? (yes/no)\n",
      "Assistant: Executing the approved plan now ✅\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langgraph langchain-core\n",
    "\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver   # <-- use a checkpointer\n",
    "from langgraph.types import interrupt, Command          # <-- Command is the key\n",
    "\n",
    "# ---------- State ----------\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    approved: Literal[\"yes\", \"no\", \"unknown\"]\n",
    "\n",
    "# ---------- Nodes ----------\n",
    "def plan(state: GraphState) -> GraphState:\n",
    "    last_user = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
    "    proposal = f\"Plan: I will carry out the request -> {last_user}. Approve? (yes/no)\"\n",
    "    return {\"messages\": [AIMessage(content=proposal)]}\n",
    "\n",
    "def need_approval(state: GraphState) -> GraphState:\n",
    "    answer = interrupt({\"question\": \"Approve the plan? Reply 'yes' or 'no'.\"})\n",
    "    ans = str(answer).strip().lower()\n",
    "    if ans not in (\"yes\", \"no\"):\n",
    "        ans = \"no\"\n",
    "    return {\"approved\": ans}\n",
    "\n",
    "def execute(state: GraphState) -> GraphState:\n",
    "    if state.get(\"approved\", \"unknown\") != \"yes\":\n",
    "        return {\"messages\": [AIMessage(\"Execution cancelled based on your decision.\")]}\n",
    "    return {\"messages\": [AIMessage(\"Executing the approved plan now ✅\")]}\n",
    "\n",
    "# ---------- Graph wiring ----------\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"plan\", plan)\n",
    "builder.add_node(\"need_approval\", need_approval)\n",
    "builder.add_node(\"execute\", execute)\n",
    "\n",
    "builder.add_edge(START, \"plan\")\n",
    "builder.add_edge(\"plan\", \"need_approval\")\n",
    "builder.add_edge(\"need_approval\", \"execute\")\n",
    "builder.add_edge(\"execute\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# ---------- Example run ----------\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "print(\"\\n--- Starting run (expect an interrupt asking for approval) ---\")\n",
    "events = list(app.stream(\n",
    "    {\"messages\": [HumanMessage(\"Book a table for two at 7pm and send me a confirmation.\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "))\n",
    "\n",
    "# Simulate human approval gathered from your UI / CLI:\n",
    "human_reply = input()\n",
    "\n",
    "print(\"\\n--- Resuming with human approval:\", human_reply, \"---\")\n",
    "# ✅ Correct way to resume:\n",
    "resumed_events = list(app.stream(Command(resume=human_reply), config, stream_mode=\"values\"))\n",
    "\n",
    "final_state = resumed_events[-1] if resumed_events else {}\n",
    "print(\"\\n--- Final assistant messages ---\")\n",
    "for msg in final_state.get(\"messages\", []):\n",
    "    role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "    print(f\"{role}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397477a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
