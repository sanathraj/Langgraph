{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b997d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is artificial intelligence?\n",
      "Answer: Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines that work and react like humans. It has been around since the 1950s and has evolved significantly over the decades. Modern AI applications include machine learning, natural language processing, computer vision, and robotics.\n",
      "Query Type: factual\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: Compare supervised and unsupervised learning\n",
      "Answer: Supervised learning and unsupervised learning are two main types of machine learning. \n",
      "\n",
      "Supervised learning, as mentioned in the context, involves training a model on labeled data where the algorithm learns to map input data to the correct output. This type of learning requires a dataset with input-output pairs for the algorithm to learn from. Examples of supervised learning algorithms include neural networks, decision trees, and support vector machines.\n",
      "\n",
      "On the other hand, unsupervised learning involves training a model on unlabeled data where the algorithm tries to find patterns or relationships in the data without explicit guidance. This type of learning is more exploratory in nature as the algorithm is left to discover the underlying structure of the data on its own. \n",
      "\n",
      "In summary, supervised learning requires labeled data for training, while unsupervised learning works with unlabeled data to find patterns or relationships.\n",
      "Query Type: analytical\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: How do large language models work?\n",
      "Answer: Large Language Models (LLMs) work by being trained on vast amounts of text data to understand and generate human-like text. They use techniques like deep learning and neural networks to process and analyze language patterns. These models, such as GPT, BERT, and Claude, have significantly advanced natural language processing capabilities and are utilized in various applications like chatbots, content generation, and language translation (Source: Context).\n",
      "Query Type: analytical\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import operator\n",
    "\n",
    "# State definition for the RAG workflow\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "    query_type: str\n",
    "    reflection: str\n",
    "    revision_needed: bool\n",
    "    final_answer: str\n",
    "\n",
    "class AgenticRAG:\n",
    "    def __init__(self, llm_model=\"gpt-3.5-turbo\", embedding_model=None):\n",
    "        self.llm = ChatOpenAI(model=llm_model, temperature=0)\n",
    "        self.embeddings = embedding_model or OpenAIEmbeddings()\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.workflow = self._build_workflow()\n",
    "        \n",
    "    def setup_vectorstore(self, documents: List[str], chunk_size=1000, chunk_overlap=200):\n",
    "        \"\"\"Initialize vector store with documents\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        doc_chunks = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            chunks = text_splitter.split_text(doc)\n",
    "            for chunk in chunks:\n",
    "                doc_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": f\"doc_{i}\"}\n",
    "                ))\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = FAISS.from_documents(doc_chunks, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        \n",
    "    def query_router(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Route query to determine the type of question\"\"\"\n",
    "        router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Analyze the following question and classify it into one of these categories:\n",
    "        - factual: Direct factual questions that can be answered from documents\n",
    "        - analytical: Questions requiring analysis or synthesis of information\n",
    "        - creative: Questions asking for creative content or opinions\n",
    "        - unclear: Questions that are ambiguous or unclear\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Respond with only the category name.\n",
    "        \"\"\")\n",
    "        \n",
    "        chain = router_prompt | self.llm | StrOutputParser()\n",
    "        query_type = chain.invoke({\"question\": state[\"question\"]})\n",
    "        \n",
    "        state[\"query_type\"] = query_type.strip().lower()\n",
    "        return state\n",
    "    \n",
    "    def retrieve_documents(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Retrieve relevant documents based on the query\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized. Call setup_vectorstore() first.\")\n",
    "            \n",
    "        # Enhance query for better retrieval\n",
    "        if state[\"query_type\"] == \"analytical\":\n",
    "            enhanced_query = f\"Analysis of {state['question']} including key factors and relationships\"\n",
    "        elif state[\"query_type\"] == \"factual\":\n",
    "            enhanced_query = state[\"question\"]\n",
    "        else:\n",
    "            enhanced_query = state[\"question\"]\n",
    "            \n",
    "        documents = self.retriever.invoke(enhanced_query)\n",
    "        state[\"documents\"] = documents\n",
    "        return state\n",
    "    \n",
    "    def grade_documents(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Grade and filter retrieved documents for relevance\"\"\"\n",
    "        grader_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a document grader. Assess whether the following document is relevant to the user question.\n",
    "        \n",
    "        Question: {question}\n",
    "        Document: {document}\n",
    "        \n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\n",
    "        Respond with only 'yes' or 'no'.\n",
    "        \"\"\")\n",
    "        \n",
    "        chain = grader_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        filtered_docs = []\n",
    "        for doc in state[\"documents\"]:\n",
    "            score = chain.invoke({\n",
    "                \"question\": state[\"question\"],\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            if score.strip().lower() == \"yes\":\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        state[\"documents\"] = filtered_docs\n",
    "        return state\n",
    "    \n",
    "    def generate_answer(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Generate answer based on retrieved documents\"\"\"\n",
    "        if not state[\"documents\"]:\n",
    "            state[\"generation\"] = \"I don't have enough relevant information to answer your question.\"\n",
    "            return state\n",
    "            \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
    "        \n",
    "        generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant tasked with answering questions based on the provided context.\n",
    "        \n",
    "        Question: {question}\n",
    "        Query Type: {query_type}\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Instructions:\n",
    "        - For factual questions: Provide direct, accurate answers based on the context\n",
    "        - For analytical questions: Synthesize information and provide insights\n",
    "        - For unclear questions: Ask for clarification while providing what information you can\n",
    "        - Always cite specific parts of the context when possible\n",
    "        - If the context doesn't contain sufficient information, clearly state this\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        \n",
    "        chain = generation_prompt | self.llm | StrOutputParser()\n",
    "        generation = chain.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"query_type\": state[\"query_type\"],\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        state[\"generation\"] = generation\n",
    "        return state\n",
    "    \n",
    "    def reflect_on_answer(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Reflect on the generated answer for quality and completeness\"\"\"\n",
    "        reflection_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a quality assessor. Evaluate the following answer for:\n",
    "        1. Accuracy based on the provided context\n",
    "        2. Completeness in addressing the question\n",
    "        3. Clarity and coherence\n",
    "        4. Proper use of context\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer: {generation}\n",
    "        Context: {context}\n",
    "        \n",
    "        Provide your assessment and suggest improvements if needed.\n",
    "        If the answer is satisfactory, respond with \"APPROVED\".\n",
    "        If improvements are needed, provide specific suggestions.\n",
    "        \"\"\")\n",
    "        \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
    "        chain = reflection_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        reflection = chain.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"generation\": state[\"generation\"],\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        state[\"reflection\"] = reflection\n",
    "        state[\"revision_needed\"] = \"APPROVED\" not in reflection.upper()\n",
    "        return state\n",
    "    \n",
    "    def revise_answer(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Revise the answer based on reflection feedback\"\"\"\n",
    "        revision_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Based on the feedback provided, revise the following answer to improve its quality.\n",
    "        \n",
    "        Original Question: {question}\n",
    "        Original Answer: {generation}\n",
    "        Feedback: {reflection}\n",
    "        Context: {context}\n",
    "        \n",
    "        Provide a revised, improved answer:\n",
    "        \"\"\")\n",
    "        \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
    "        chain = revision_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        revised_answer = chain.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"generation\": state[\"generation\"],\n",
    "            \"reflection\": state[\"reflection\"],\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        state[\"final_answer\"] = revised_answer\n",
    "        return state\n",
    "    \n",
    "    def finalize_answer(self, state: RAGState) -> RAGState:\n",
    "        \"\"\"Finalize the answer (use original if no revision needed)\"\"\"\n",
    "        if not state[\"revision_needed\"]:\n",
    "            state[\"final_answer\"] = state[\"generation\"]\n",
    "        return state\n",
    "    \n",
    "    def should_revise(self, state: RAGState) -> str:\n",
    "        \"\"\"Decide whether to revise the answer\"\"\"\n",
    "        return \"revise\" if state.get(\"revision_needed\", False) else \"finalize\"\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph workflow\"\"\"\n",
    "        workflow = StateGraph(RAGState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"query_router\", self.query_router)\n",
    "        workflow.add_node(\"retrieve\", self.retrieve_documents)\n",
    "        workflow.add_node(\"grade_docs\", self.grade_documents)\n",
    "        workflow.add_node(\"generate\", self.generate_answer)\n",
    "        workflow.add_node(\"reflect\", self.reflect_on_answer)\n",
    "        workflow.add_node(\"revise\", self.revise_answer)\n",
    "        workflow.add_node(\"finalize\", self.finalize_answer)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.set_entry_point(\"query_router\")\n",
    "        workflow.add_edge(\"query_router\", \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"grade_docs\")\n",
    "        workflow.add_edge(\"grade_docs\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", \"reflect\")\n",
    "        \n",
    "        # Conditional edge for revision\n",
    "        workflow.add_conditional_edges(\n",
    "            \"reflect\",\n",
    "            self.should_revise,\n",
    "            {\n",
    "                \"revise\": \"revise\",\n",
    "                \"finalize\": \"finalize\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"revise\", \"finalize\")\n",
    "        workflow.add_edge(\"finalize\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Execute the RAG workflow for a given question\"\"\"\n",
    "        initial_state = {\n",
    "            \"question\": question,\n",
    "            \"documents\": [],\n",
    "            \"generation\": \"\",\n",
    "            \"query_type\": \"\",\n",
    "            \"reflection\": \"\",\n",
    "            \"revision_needed\": False,\n",
    "            \"final_answer\": \"\"\n",
    "        }\n",
    "        \n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the RAG system\n",
    "    rag_system = AgenticRAG()\n",
    "    \n",
    "    # Example documents (replace with your actual documents)\n",
    "    sample_documents = [\n",
    "        \"\"\"\n",
    "        Artificial Intelligence (AI) is a branch of computer science that aims to create \n",
    "        intelligent machines that work and react like humans. AI has been around since \n",
    "        the 1950s and has evolved significantly over the decades. Modern AI applications \n",
    "        include machine learning, natural language processing, computer vision, and robotics.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Machine Learning is a subset of AI that enables computers to learn and improve \n",
    "        from experience without being explicitly programmed. There are three main types \n",
    "        of machine learning: supervised learning, unsupervised learning, and reinforcement \n",
    "        learning. Popular algorithms include neural networks, decision trees, and support vector machines.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Large Language Models (LLMs) are AI systems trained on vast amounts of text data \n",
    "        to understand and generate human-like text. Examples include GPT, BERT, and Claude. \n",
    "        These models have revolutionized natural language processing and are used in \n",
    "        chatbots, content generation, and language translation.\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Setup vector store\n",
    "    rag_system.setup_vectorstore(sample_documents)\n",
    "    \n",
    "    # Example queries\n",
    "    questions = [\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Compare supervised and unsupervised learning\",\n",
    "        \"How do large language models work?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        result = rag_system.query(question)\n",
    "        print(f\"Answer: {result['final_answer']}\")\n",
    "        print(f\"Query Type: {result['query_type']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b55e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
