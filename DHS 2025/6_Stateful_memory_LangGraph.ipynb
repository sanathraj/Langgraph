{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d55ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794d3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MEMORY STORE ----------\n",
    "# This is shared across users\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Add user-specific context into store\n",
    "store.put(\n",
    "    namespace=(\"user123\", \"chitchat\"),\n",
    "    key=\"profile\",\n",
    "    value={\n",
    "        \"rules\": [\n",
    "            \"User likes short, direct language\",\n",
    "            \"User only speaks English & Python\"\n",
    "        ],\n",
    "        \"name\": \"Alice\",\n",
    "    },\n",
    ")\n",
    "\n",
    "store.put(\n",
    "    namespace=(\"user456\", \"chitchat\"),\n",
    "    key=\"profile\",\n",
    "    value={\n",
    "        \"rules\": [\"User prefers concise answers\"],\n",
    "        \"name\": \"Bob\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627ae023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, \"The conversation so far\"]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d66b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent_node(state: State, config):\n",
    "    \"\"\"Instead of hardcoding logic, just inject memory into the LLM prompt\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    namespace = (user_id, \"chitchat\")\n",
    "\n",
    "    user_profile = store.get(namespace, \"profile\") or {}\n",
    "    memory_context = f\"User profile: {user_profile}\"\n",
    "\n",
    "    # Let the LLM decide the answer\n",
    "    response = await llm.ainvoke(\n",
    "        state[\"messages\"] + [HumanMessage(content=memory_context)]\n",
    "    )\n",
    "\n",
    "    return {\"messages\": state[\"messages\"] + [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22a6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcb5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User123 ---\n",
      "Your name is Alice.\n",
      "Based on your profile, you like short and direct language. You also communicate in English and Python. If there's anything specific you'd like to discuss or know more about, just let me know!\n",
      "\n",
      "--- User456 ---\n",
      "Your name is Bob.\n",
      "Hi Bob! Since you prefer concise answers, I'll keep it brief. If you'd like to tell me about your interests or hobbies, I can help you figure out what you like! Otherwise, I can provide some common interests if that helps.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    print(\"\\n--- User123 ---\")\n",
    "    result = await graph.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "        config={\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 1}}\n",
    "    )\n",
    "    print(result[\"messages\"][-1].content)\n",
    "\n",
    "    result = await graph.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What do I like?\")]},\n",
    "        config={\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 1}}\n",
    "    )\n",
    "    print(result[\"messages\"][-1].content)\n",
    "\n",
    "    print(\"\\n--- User456 ---\")\n",
    "    result = await graph.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "        config={\"configurable\": {\"user_id\": \"user456\", \"thread_id\": 1}}\n",
    "    )\n",
    "    print(result[\"messages\"][-1].content)\n",
    "\n",
    "    result = await graph.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=\"What do I like?\")]},\n",
    "        config={\"configurable\": {\"user_id\": \"user456\", \"thread_id\": 1}}\n",
    "    )\n",
    "    print(result[\"messages\"][-1].content)\n",
    "\n",
    "await(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d67401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
